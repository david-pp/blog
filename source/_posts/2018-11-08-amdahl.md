---
title: 阿姆达尔定律：做优化的大逻辑
date: 2018-11-08 21:00:00
category : 
tags: []
---

经常我们会做各种优化，神马程序性能优化，编译效率优化，开发流程优化，产品功能优化，等等，试问有啥不可以优化的，包括我们自己本身在各个方面也可以进行优化。怎么进行优化，优化什么，有没有大的指导逻辑了？

最近看了一篇文章受益匪浅，计算机科学家阿姆达尔给出一个做优化的大逻辑。说白了，道理很简单，也很符合我们的直觉，科学家为啥牛逼，就是可以把一些浅显的道理构建一个数学模型，进行量化，让你举棋不定的时候，给出选择。（PS.懒得看公式的同学，直接跳到末尾看结论）

阿姆达尔（Amdahl）何许人也？IBM大型机系统System/360之父，在做大型机这么复杂的系统的时候，在不断对系统进行性能提升时，总结出来一个经验公式：

![Amdahl](/images/Amdahl-2.png)

其中：

- S - 性能提升（加速）
- s - 系统中某一项指标的提升
- p - 这项指标在整个系统/流程被用到的占比

考虑一下极限情况（假设该项指标可以被提升到无限大），结果如下：

![Amdahl](/images/Amdahl-3.png)


公式看完，我们来考虑一个老爷子老本行的一个例子：

> 要做一个计算机系统的优化，简单点只涉及到内存和CPU，假设执行一些任务时，CPU用到90％，内存10％。

对CPU进行多核化，假设分别为4核、8核、32核、128核、512核，来通过阿姆达尔公式计算下加速比：

  - S(4) = 1/(1-0.9 + 0.9/4) = 3.08
  - S(8) = 1/(1-0.9 + 0.9/8) = 4.71
  - S(32) = 1/(1-0.9 + 0.9/32) = 7.80
  - S(128) = 1/(1-0.9 + 0.9/128) = 9.34
  - S(512) = 1/(1-0.9 + 0.9/512) = 9.83
  
曲线可以参考下面紫色那条：

![Amdahl](/images/Amdahl-1.png)

可以看到开始多增加几个核心，效率线性提升，但是随着核心超过128，效率提升就非常不明显，就算你很牛逼，把无穷个核心都挂上去，效率提升也就那样了，趋近极限值：1/(1-0.9）= 10。（这也是为啥这几年摩尔定律失灵之后，计算机采用多核CPU技术提升效率，但是靠多核也是有极限的，现在很少有64核心以上的机器吧？为啥，加再多CPU核心，毛用都没）

难道计算机系统就这样了没有提升空间了吗？当然不是，问题的关键点就转移到CPU用到的百分比了，假设去优化内存，把内存做的存取速度超过，执行某些任务的时候，内存用的比例低了，CPU的比例提上去了，假设为95%，看下上图绿色和紫色的对比就知道效率的差异了。


此刻，把老爷子的公式推及到所有要做优化的事情，把刚才的例子和我们要做的优化做类比，你会发现，我去，还真就是那么回事。

假设要做开发流程的优化，第一个要优化的是什么？假设程序猿有下面三个活动要进行优化：编译、部署、测试。编译要用60分钟，部署20分钟，测试40分钟。假设把编译时间和部署时间缩小到一半要花的成本相同，那么选择优化编译肯定是没错的。根据阿姆达尔公式进行计算，提升编译速度效率可以提升1.4倍=1/(1-0.6 + 0.6/2)，而去优化部署则只能提升1.11倍=1/(1-0.2 + 0.2/2)。和我们的直觉是一致的：**同等成本的提升，优化当然要找占比大的了**。

下面这个就不太好根据直觉判定了，假设编译时间减半和部署优化到0.2分钟（效率100倍提升），花费的代价是一样的，这个时候怎么选择？我们直觉往往会选择一些看似提升比例很大，或者容易做的事情，结果会是怎样了？部署效率提升100倍，总的提升是1.24倍=1/(1-0.2 + 0.2/100)，比起编译速度减半的提升1.4还是要少一些的。

综上，根据阿姆达尔定律，我们可以得出一些做优化的大逻辑：

- 做优化，你首先得分解，找到可以优化的点，分析各个点相对于整个系统的占比。
- 花费同等时间和精力，选择占比最大的点去优化。
- 对于系统来说，某个点的优化是有极限的，到了一定程度，再多努力都是无用功。
- 仔细评估各个点优化的代价，选择性价比最高的（Amdahl定律），而不是挑软柿子捏，否则自我感觉良好，屁用没有。
- ...

可以试着把阿姆达尔提供的这个大逻辑，类比到自我提升、程序性能优化等等。大道至简，好玩！










  






